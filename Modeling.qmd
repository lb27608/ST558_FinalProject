---
title: "ST 558 Final Project: Modeling"
author: "Lee Bennett"
format: html
editor: visual
---

```{r load packages}
#| include: false

library(readr)
library(tidyverse)
library(purrr)
library(tidymodels)
library(tree)
library(ranger)
```

## Introduction

The purpose of this analysis is to create a predictive model for the incidence of diabetes using data from the 2015 [Behavioral Risk Factor Surveillance System (BRFSS)](https://www.cdc.gov/brfss/index.html) survey conducted by the Centers for Disease Control. The dataset we will use for modeling includes survey responses from 253,680 individuals.

The response variable in the dataset, `Diabetes_binary`, is a 0/1 indicator for the presence of diabetes or prediabetes. There are a number of potential predictors in the dataset, but we will consider only the following health-related variables in the dataset:

-   Binary factors for hypertension and high cholesterol (yes vs. no)
-   Reported body mass index (BMI)
-   Sex (male vs. female)
-   A factor for having any kind of health insurance coverage (yes vs. no)
-   An ordinal factor for self-rated general health, ranging from 1 ("Excellent") to 5 ("Poor")


## Analysis Data

As we did for the exploratory data analysis, we'll read in the data from the raw CSV file and transform the predictor variables as needed:

```{r create data}
#| warning: false

#Read in the raw data
diabetes_raw <- read_csv(file="diabetes_binary_health_indicators_BRFSS2015.csv",show_col_types=FALSE)

#Create an analysis dataset containing only the relevant variables
yn_label <- c("No","Yes")
diabetes_ad <- diabetes_raw |> select(Diabetes_binary, HighBP ,HighChol, BMI, Smoker, Sex, AnyHealthcare, GenHlth) |>
               mutate(diabetes = factor(Diabetes_binary, levels=c(0,1), labels=c("No diabetes", "Diabetes or prediabetes")))
```

For modeling, we'll do an initial split of the full dataset into training (70%) and test (30%) sets, and then create a 5-fold cross-validation dataset using the training data:

```{r split data}
#Create initial split
set.seed(1434)
diab_split <- initial_split(diabetes_ad, prop=0.70)
diab_train <- training(diab_split)
diab_test <- testing(diab_split)

#Create 5-fold cross-validation sets on the training data
diab_5_fold <- vfold_cv(diab_train,5)
```

## Modeling

We'll consider both a classification tree model and a random forest model for prediction of diabetes/prediabetes using the variables we have selected.

### Classification Tree

The classification tree model predicts the outcome-in this case, whether or not the subject has diabetes or prediabetes-by splitting the space of predictor variables into regions on step at a time. To create branches, the modeling process considers splits of each potential predictor and selects the one that produces the best separation of the outcome variable. This process is repeated for each new branch of the tree until the algorithm terminates. 

```{r Classification tree model}
tree_spec <- decision_tree(tree_depth = 8,
                           cost_complexity = tune()) |>
             set_engine("rpart") |> set_mode("classification")

#Create the model recipe
tree_recipe <- recipe(diabetes ~ ., data = diab_train) |>
               step_mutate(high_bp = factor(HighBP, levels=c(0,1), labels=yn_label),
                           high_chol = factor(HighChol, levels=c(0,1), labels=yn_label),
                           sex = factor(Sex, levels=c(0,1), labels=c("Female","Male")),
                           smoker = factor(Smoker, levels=c(0,1), labels=c("Less than 100 packs or none", "At least 100 packs")),
                           healthcare = factor(AnyHealthcare, levels=c(0,1), labels=yn_label),
                           gen_health = factor(GenHlth, levels=c(1,2,3,4,5), labels=c("Excellent", "Very good", "Good", "Fair", "Poor"))) |>
               step_normalize(BMI)
prep(tree_recipe)

#Create the workflow
tree_wkf <- workflow() |> add_recipe(tree_recipe) |> add_model(tree_spec)

#Fit the model to the CV folds to select tuning parameters
tree_grid <- tree_wkf |> tune_grid(resamples=diab_5_fold, 
                                   grid=grid_regular(cost_complexity(),
                                   levels=5),
                                   metrics=metric_set(mn_log_loss))

#Collect metrics and choose model with lowest log loss
tree_best_params <- select_best(tree_grid, metric="mn_log_loss")
tree_best_params
```

### Random Forest Model

<Add description here>

```{r Random forest model}
rf_spec <- rand_forest(mtry = tune()) |> set_engine("ranger") |>
set_mode("classification")

#Create the workflow using the same model recipe as before
rf_wkf <- workflow() |> add_recipe(tree_recipe) |> add_model(rf_spec)

#Fit the model to the CV folds to select tuning parameter for mtry
rf_grid <- rf_wkf |> tune_grid(resamples = diab_5_fold,
                               grid = 7,
                               metrics = metric_set(mn_log_loss))

#Collect metrics and choose model with lowest log loss, then display the tuning params
rf_best_params <- select_best(rf_grid, metric="mn_log_loss")
rf_best_params
```
Among the grid of values that was searched, the best value of `mtry` was 3.

### Comparing Models

To select the best model, we'll compare the final classification tree and random forest models by checking their performance on the test portion of the data. The model with the smallest log loss will be the one that we use in the development of the API in the next step of the project.

```{r compare models}

#Finalize the workflows and run the final models on the full training set
tree_final_wkf <- tree_wkf |> finalize_workflow(tree_best_params)
tree_final_fit <- tree_final_wkf |> last_fit(diab_split, metrics=metric_set(mn_log_loss))
rf_final_wkf <- rf_wkf |> finalize_workflow(rf_best_params)
rf_final_fit <- rf_final_wkf |> last_fit(diab_split, metrics=metric_set(mn_log_loss))

tree_final_fit |> collect_metrics()
rf_final_fit |> collect_metrics()
```
The log loss values for the final tree model and final random forest model are 0.333 and 0.323, respectively, so we will use the random forest model in our API.


